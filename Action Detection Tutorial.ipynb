{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "\n",
    "#         # Read feed\n",
    "#         ret, frame = cap.read()\n",
    "\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "#         print(results)\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_styled_landmarks(image, results)\n",
    "\n",
    "#         # Show to screen\n",
    "#         cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#         # Break gracefully\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if results.left_hand_landmarks:\n",
    "#     print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(results.left_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose = []\n",
    "# for res in results.pose_landmarks.landmark:\n",
    "#     test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "#     pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "# face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "# lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "# rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 468*3+33*4+21*3+21*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load('0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that we try to detect\n",
    "# actions = ['Bye', 'Done', 'Hello', 'How are you', 'I am Fine', 'Nice to meet you', 'Please', 'See you later', 'Sorry', 'Thanks', 'Welcome', 'Nothing', \"what's up\", 'Where are you', 'Home']\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# for action in actions:\n",
    "#     for sequence in range(no_sequences):\n",
    "#         try: \n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MP_Data\\\\Home\\\\0\\\\0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m extract_keypoints(results)\n\u001b[0;32m     36\u001b[0m npy_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH, action, \u001b[38;5;28mstr\u001b[39m(sequence), \u001b[38;5;28mstr\u001b[39m(frame_num))\n\u001b[1;32m---> 37\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpy_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Break gracefully\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msave\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\numpy\\lib\\npyio.py:515\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    514\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 515\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    518\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MP_Data\\\\Home\\\\0\\\\0.npy'"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "#             Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "#                 print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = os.listdir(DATA_PATH)\n",
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bye': 0,\n",
       " 'Done': 1,\n",
       " 'Hello': 2,\n",
       " 'How are you': 3,\n",
       " 'I am Fine': 4,\n",
       " 'Nice to meet you': 5,\n",
       " 'No': 6,\n",
       " 'Please': 7,\n",
       " 'See you later': 8,\n",
       " 'Sorry': 9,\n",
       " 'Thanks': 10,\n",
       " 'Welcome': 11,\n",
       " 'What are you doing': 12,\n",
       " 'What is your Name': 13,\n",
       " \"what's up\": 14,\n",
       " 'Where are you': 15,\n",
       " 'Who are you': 16,\n",
       " 'Yes': 17}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540, 30, 1662)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540, 30, 1662)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(np.array(actions).shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "17/17 [==============================] - 7s 101ms/step - loss: 3.7923 - categorical_accuracy: 0.0507\n",
      "Epoch 2/2000\n",
      "17/17 [==============================] - 2s 98ms/step - loss: 2.9206 - categorical_accuracy: 0.0663\n",
      "Epoch 3/2000\n",
      "17/17 [==============================] - 2s 98ms/step - loss: 2.8103 - categorical_accuracy: 0.1072\n",
      "Epoch 4/2000\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 2.8859 - categorical_accuracy: 0.0877\n",
      "Epoch 5/2000\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 2.8979 - categorical_accuracy: 0.0994\n",
      "Epoch 6/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 2.8269 - categorical_accuracy: 0.0955\n",
      "Epoch 7/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 2.7239 - categorical_accuracy: 0.1715\n",
      "Epoch 8/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 2.6144 - categorical_accuracy: 0.1676\n",
      "Epoch 9/2000\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 2.5700 - categorical_accuracy: 0.1735\n",
      "Epoch 10/2000\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 2.4376 - categorical_accuracy: 0.2144\n",
      "Epoch 11/2000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 2.3218 - categorical_accuracy: 0.2125\n",
      "Epoch 12/2000\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 2.7574 - categorical_accuracy: 0.1053\n",
      "Epoch 13/2000\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 2.9245 - categorical_accuracy: 0.1092\n",
      "Epoch 14/2000\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 2.9273 - categorical_accuracy: 0.1111\n",
      "Epoch 15/2000\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 2.6411 - categorical_accuracy: 0.1579\n",
      "Epoch 16/2000\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 2.6121 - categorical_accuracy: 0.1520\n",
      "Epoch 17/2000\n",
      "17/17 [==============================] - 2s 100ms/step - loss: 2.5710 - categorical_accuracy: 0.1793\n",
      "Epoch 18/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\users\\shree\\desktop\\python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 18)                594       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 597,170\n",
      "Trainable params: 597,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[23,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[24,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[24,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[25,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[24,  0],\n",
       "        [ 0,  3]],\n",
       "\n",
       "       [[26,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[25,  0],\n",
       "        [ 0,  2]],\n",
       "\n",
       "       [[26,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[23,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[26,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[26,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[26,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[26,  0],\n",
       "        [ 0,  1]]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "    \n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('MP_Data') \n",
    "no_sequences = 30\n",
    "sequence_length = 30\n",
    "actions = os.listdir(DATA_PATH)\n",
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(np.array(actions).shape[0], activation='softmax'))\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(\"chromedriver.exe\")\n",
    "path = \"C:\\\\Users\\\\shree\\\\Downloads\\\\chromedriver.exe\"\n",
    "linkurl = ''\n",
    "url = \"https://sign-language.ai4bharat.org/#/dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI\n",
    "def normal():\n",
    "\troot = customtkinter.CTk()  # create CTk window like the Tk window\n",
    "\troot.title(\"Chat App\")\n",
    "\n",
    "\t# Send function\n",
    "\tdef send():\n",
    "\t\ttext = entry.get()\t\n",
    "\t\tdata = {\"Normal\":text}\n",
    "\t\tdb.child(\"users\").push(data)\n",
    "\t\tfun()\n",
    "\t\tentry.delete(0, END)\n",
    "\n",
    "\tlabel = customtkinter.CTkLabel(master=root,\n",
    "                               text=\"ISL Action Detection\",\n",
    "                               width=120,\n",
    "                               height=25,\n",
    "                               corner_radius=8,\n",
    "\t\t\t\t\t\t\t   text_font=('Arial',30))\n",
    "\tlabel.place(relx=0.5, rely=0.05, anchor=tkinter.CENTER)\n",
    "\n",
    "\tframe = customtkinter.CTkFrame(master=root,\n",
    "                               width=550,\n",
    "                               height=340,\n",
    "                               corner_radius=10)\n",
    "\tframe.place(relx=0.5, rely=0.45, anchor=tkinter.CENTER)\n",
    "\n",
    "\t# txt = Text(root,bg='#242526',fg='white',width=65,height=20)\n",
    "\t# txt.grid(row=1, column=0, columnspan=2)\n",
    "\n",
    "\tentry = customtkinter.CTkEntry(master=root,\n",
    "\t\t\t\t\t\t\t   height=50,\n",
    "\t\t\t\t\t\t\t   width=390,\n",
    "                               corner_radius=10)\n",
    "\tentry.place(relx=0.39, rely=0.9, anchor=tkinter.CENTER)\n",
    "\n",
    "\tscrollbar = Scrollbar(frame)\n",
    "\ttxt = Text(frame,bg='#242526',fg='white',width=65,height=20,yscrollcommand=scrollbar.set)\n",
    "\tscrollbar.config(command=txt.yview)\n",
    "\tscrollbar.pack(side=RIGHT, fill=Y)\n",
    "\ttxt.pack()\n",
    "\n",
    "\t# lable1 = Label(root, bg=BG_COLOR, fg=TEXT_COLOR, text=\"ISL Action Detection\", font=FONT_BOLD, pady=10, width=20, height=1).grid(\n",
    "\t# \trow=0)\n",
    "\n",
    "\t# txt = Text(root, bg=BG_COLOR, fg=TEXT_COLOR, font=FONT, width=60)\n",
    "\t# txt.grid(row=1, column=0, columnspan=2)\n",
    "\n",
    "\t# e = Entry(root, bg=\"#2C3E50\", fg=TEXT_COLOR, font=FONT, width=55)\n",
    "\t# e.grid(row=2, column=0)\n",
    "\n",
    "\t# send = Button(root, text=\"Send\", font=FONT_BOLD, bg=BG_GRAY, foreground= 'green',\n",
    "\t# \t\t\t\tcommand=send).grid(row=2, column=1)\n",
    "\n",
    "\tbutton = customtkinter.CTkButton(master=root, fg_color=\"black\",height=50,width=70,text=\"Send\",command=send)\n",
    "\tbutton.place(relx=0.80, rely=0.9, anchor=tkinter.CENTER)\n",
    "\n",
    "\tdef fun():\n",
    "\t\ttxt.delete(\"1.0\",\"end\")\n",
    "\t\tall_users = db.child(\"users\").get()\n",
    "\t\tif all_users.each():\n",
    "\t\t\tfor user in all_users.each():\n",
    "\t\t\t\t# val = user.val()[user.key()]\n",
    "\t\t\t\tfor key, value in user.val().items():\n",
    "\t\t\t\t\ttxt.insert(END, \"\\n\\n\" + key+\" -> \"+value) \t\t\n",
    "\t\t\t\t# print(user.key())\n",
    "\n",
    "\tbutton = customtkinter.CTkButton(master=root, fg_color=\"black\",height=50,width=65,text=\"↻\",command=fun)\n",
    "\tbutton.place(relx=0.93, rely=0.9, anchor=tkinter.CENTER)\n",
    "\n",
    "\tfun()\n",
    "\troot.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deaf():\n",
    "    root = Toplevel()  # create CTk window like the Tk window\n",
    "    root.title(\"Chat App\")\n",
    "    root.geometry(\"1400x700\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    #  1. New detection variables\n",
    "    def ActionDetection():\n",
    "        sequence = []\n",
    "        sentence = []\n",
    "        temp = [[],[]]\n",
    "        lst = []\n",
    "        num = 0\n",
    "        threshold = 0.9\n",
    "\n",
    "        # Set mediapipe model \n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "            while cap.isOpened():\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                # 2. Prediction logic\n",
    "                keypoints = extract_keypoints(results)\n",
    "            #         sequence.insert(0,keypoints)\n",
    "            #         sequence = sequence[:30]\n",
    "                sequence.append(keypoints)\n",
    "                sequence = sequence[-30:]\n",
    "\n",
    "                if len(sequence) == 30:\n",
    "                    res = model.predict(np.expand_dims(sequence, axis=0))[0] \n",
    "\n",
    "                    if results.left_hand_landmarks or results.right_hand_landmarks:        \n",
    "                        #3. Viz logic\n",
    "                            if res[np.argmax(res)] > threshold: \n",
    "                #                 if len(sentence) > 0: \n",
    "                #                     if actions[np.argmax(res)] != sentence[-1]:\n",
    "                #                         sentence.append(actions[np.argmax(res)])\n",
    "                #                 else:\n",
    "                                temp[0].append(actions[np.argmax(res)])\n",
    "                                temp[1].append(res[np.argmax(res)])\n",
    "\n",
    "\n",
    "                            if len(temp[0]) > 4: \n",
    "                                pos = temp[1].index(max(temp[1]))\n",
    "                                sentence.append(temp[0][pos])\n",
    "                                sentence = sentence[-1:]\n",
    "                                temp = [[],[]]\n",
    "                                num=1\n",
    "                                \n",
    "                    elif num==1:\n",
    "                        lst.append(results.face_landmarks)\n",
    "                        if len(lst)==5 and lst[4]:\n",
    "#                             print(sentence)\n",
    "                            text = sentence[0]\n",
    "                            data = {\"deaf\":text}\n",
    "                            db.child(\"users\").push(data)\n",
    "                            num=0\n",
    "                            lst=[]\n",
    "                        elif len(lst)>5:\n",
    "                            num=0\n",
    "                            lst=[]\n",
    "                        \n",
    "                img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                img = ImageTk.PhotoImage(Image.fromarray(img))\n",
    "                label1['image'] = img\n",
    "                label3.delete(\"1.0\",\"end\")\n",
    "                label3.tag_configure(\"center\", justify='center')\n",
    "                label3.insert(END,'\\n'+' '.join(sentence))\n",
    "                label3.tag_add(\"center\", \"1.0\", \"end\")\n",
    "                \n",
    "#                 print(sentence)\n",
    "#                 label2['image'] = img\n",
    "                root.update()\n",
    "\n",
    "\n",
    "                    # Viz probabilities\n",
    "        #             image = prob_viz(res, actions, image, colors)\n",
    "\n",
    "#                 cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "#                 cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Show to screen\n",
    "    #             cv2.imshow('OpenCV Feed', image)\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "# \t# Send function\n",
    "        \n",
    "    def fun():\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    label = customtkinter.CTkLabel(master=root,\n",
    "                               text=\"ISL Action Detection\",\n",
    "                               width=120,\n",
    "                               height=25,\n",
    "                               corner_radius=8,\n",
    "                               text_font=('Arial',30))\n",
    "    label.place(relx=0.5, rely=0.05, anchor=tkinter.CENTER)\n",
    "    \n",
    "    label1 = customtkinter.CTkLabel(master=root,\n",
    "                               width=550,\n",
    "                               height=350,\n",
    "                               corner_radius=10)\n",
    "    label1.place(relx=0.25, rely=0.45, anchor=tkinter.CENTER) \n",
    "#     label1 = Label(frame1,bg='#242526')\n",
    "#     label1.pack()\n",
    "\n",
    "    def open_browser(value):\n",
    "    # name = input('enter name')\n",
    "        try:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"headless\")\n",
    "            driver = webdriver.Chrome(service=s,chrome_options=options)\n",
    "            driver.get(url)\n",
    "            progressfun(0.4)\n",
    "            time.sleep(5)\n",
    "            driver.find_element_by_id('dict_searchbar').send_keys(value)\n",
    "            driver.find_element_by_id('dict_searchbar').click()\n",
    "            progressfun(0.6)\n",
    "            # text_field = driver.find_element_by_id('dict_result')\n",
    "            time.sleep(5)\n",
    "            text = driver.find_elements_by_tag_name('a')\n",
    "            progressfun(0.8)\n",
    "#             print(text)\n",
    "            # text = wait(driver, 10).until(lambda driver: text_field.get_attribute('li'))\n",
    "            for temp in text:\n",
    "                link = temp.get_attribute('href')\n",
    "                if 'www.youtube.com' in  link:\n",
    "                    global linkurl\n",
    "                    linkurl = link\n",
    "#                     url   = \"https://www.youtube.com/watch?v=95DQmMXAzYk\"\n",
    "                    break\n",
    "            driver.quit()\n",
    "        except:\n",
    "            driver.quit()\n",
    "   \n",
    "    def SearchMsg():\n",
    "        all_users = db.child(\"users\").get()\n",
    "        if all_users.each():\n",
    "            num = 0\n",
    "            for user in all_users.each():\n",
    "                        # val = user.val()[user.key()]\n",
    "                for key, value in user.val().items():\n",
    "                    num+=1\n",
    "                    if key == 'Normal' and len(all_users.each())==num:\n",
    "                        progressfun(0.2)\n",
    "                        open_browser(value)\n",
    "                                          \n",
    "    label2 = customtkinter.CTkLabel(master=root,\n",
    "                               width=520,\n",
    "                               height=370,\n",
    "                               corner_radius=10)\n",
    "    label2.place(relx=0.75, rely=0.45, anchor=tkinter.CENTER)\n",
    "#     label2 = Label(frame2,bg='#242526')\n",
    "#     label2.pack()\n",
    "\n",
    "    progressbar = '' \n",
    "    def progressfun(prognum):\n",
    "        global progressbar\n",
    "        progressbar.set(prognum,True)\n",
    "        progressbar.update()\n",
    "         \n",
    "    def createprogressbar():\n",
    "        global progressbar\n",
    "        progressbar = customtkinter.CTkProgressBar(master=label2,\n",
    "                                               width=160,\n",
    "                                               height=20,\n",
    "                                               border_width=5,\n",
    "                                               progress_color='green')\n",
    "        progressbar.place(relx=0.5, rely=0.5, anchor=tkinter.CENTER)\n",
    "\n",
    "    frame = customtkinter.CTkFrame(master=root,corner_radius=10)\n",
    "    frame.place(relx=0.25, rely=0.9, anchor=tkinter.CENTER)\n",
    "    label3 =  Text(frame,bg='#242526',fg='white',height=3,width=30,font=('Arial',12))\n",
    "    label3.pack()\n",
    "\n",
    "    button = customtkinter.CTkButton(master=root, fg_color=\"black\",height=50,width=65,text=\"CLOSE\",command=fun)\n",
    "    button.place(relx=0.5, rely=0.9, anchor=tkinter.CENTER)\n",
    "        \n",
    "    def play():\n",
    "        global progressbar\n",
    "        label2['image'] = ''\n",
    "        root.update()\n",
    "        createprogressbar()\n",
    "        progressfun(0)\n",
    "        SearchMsg()\n",
    "        progressfun(1)\n",
    "        global linkurl     \n",
    "        video = pafy.new(linkurl)\n",
    "        best  = video.getbest() \n",
    "        capture = cv2.VideoCapture(best.url)\n",
    "        progressbar.destroy()\n",
    "        while capture.isOpened():\n",
    "            check, frame = capture.read()\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (660,480), interpolation = cv2.INTER_AREA)\n",
    "            img = ImageTk.PhotoImage(Image.fromarray(img))\n",
    "            label2['image'] = img\n",
    "            root.update()\n",
    "        \n",
    "        \n",
    "    btn = customtkinter.CTkButton(master=root, fg_color=\"black\",height=50,width=65,text=\"PLAY\",command=play)\n",
    "    btn.place(relx=0.75, rely=0.9, anchor=tkinter.CENTER)\n",
    "    \n",
    "    ActionDetection()\n",
    "            \n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_1888\\2427565470.py:123: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(service=s,chrome_options=options)\n",
      "C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_1888\\2427565470.py:127: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('dict_searchbar').send_keys(value)\n",
      "C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_1888\\2427565470.py:128: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_id('dict_searchbar').click()\n",
      "C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_1888\\2427565470.py:132: DeprecationWarning: find_elements_by_tag_name is deprecated. Please use find_elements(by=By.TAG_NAME, value=name) instead\n",
      "  text = driver.find_elements_by_tag_name('a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/embed/XxuhRhf5AZM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\site-packages\\customtkinter\\widgets\\ctk_button.py\", line 390, in clicked\n",
      "    self.function()\n",
      "  File \"C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_1888\\2427565470.py\", line 200, in play\n",
      "    video = pafy.new(linkurl)\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\site-packages\\pafy\\pafy.py\", line 124, in new\n",
      "    return Pafy(url, basic, gdata, size, callback, ydl_opts=ydl_opts)\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\site-packages\\pafy\\backend_youtube_dl.py\", line 31, in __init__\n",
      "    super(YtdlPafy, self).__init__(*args, **kwargs)\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\site-packages\\pafy\\backend_shared.py\", line 62, in __init__\n",
      "    self.videoid = extract_video_id(video_url)\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\site-packages\\pafy\\backend_shared.py\", line 51, in extract_video_id\n",
      "    raise ValueError(err % url)\n",
      "ValueError: Need 11 character video id or the URL of the video. Got https://www.youtube.com/embed/XxuhRhf5AZM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=-p-Uj-p_Gso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\site-packages\\customtkinter\\widgets\\ctk_button.py\", line 390, in clicked\n",
      "    self.function()\n",
      "  File \"C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_1888\\2427565470.py\", line 206, in play\n",
      "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
      "cv2.error: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=-p-Uj-p_Gso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"c:\\users\\shree\\desktop\\python\\lib\\site-packages\\customtkinter\\widgets\\ctk_button.py\", line 390, in clicked\n",
      "    self.function()\n",
      "  File \"C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_1888\\2427565470.py\", line 206, in play\n",
      "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
      "cv2.error: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from turtle import bgcolor\n",
    "from matplotlib.pyplot import margins\n",
    "import pyrebase\n",
    "import tkinter\n",
    "import customtkinter\n",
    "from PIL import Image,ImageTk\n",
    "\n",
    "timeout = 60.0 # Sixty seconds\n",
    "\n",
    "Config = {\n",
    "\t\"apiKey\": \"AIzaSyAFaPJV_f9rTlcmYVVFc2KQf8sDzGbrEGo\",\n",
    "\t\"authDomain\": \"islactiondetection.firebaseapp.com\",\n",
    "\t\"databaseURL\":\"https://islactiondetection-default-rtdb.firebaseio.com\",\n",
    "\t\"projectId\": \"islactiondetection\",\n",
    "\t\"storageBucket\": \"islactiondetection.appspot.com\",\n",
    "\t\"messagingSenderId\": \"774058191769\",\n",
    "\t\"appId\": \"1:774058191769:web:cdc9e23d6275e344144108\",\n",
    "\t\"measurementId\": \"G-B5VMSKE98B\"\n",
    "\t\t\t}\n",
    "firebase = pyrebase.initialize_app(Config)\n",
    "db = firebase.database()\n",
    "\n",
    "customtkinter.set_appearance_mode(\"Dark\")\n",
    "customtkinter.set_default_color_theme(\"blue\")\n",
    "\n",
    "root_tk = customtkinter.CTk()  # create CTk window like the Tk window\n",
    "root_tk.title(\"Chat App\")\n",
    "root_tk.geometry(\"400x240\")\n",
    "\n",
    "button1 = customtkinter.CTkButton(master=root_tk, fg_color=\"black\",text=\"Normal\",command=normal)\n",
    "button1.place(relx=0.5, rely=0.35, anchor=tkinter.CENTER)\n",
    "\n",
    "button2 = customtkinter.CTkButton(master=root_tk, fg_color=\"black\",text=\"Deaf\",command=deaf)\n",
    "button2.place(relx=0.5, rely=0.65, anchor=tkinter.CENTER)\n",
    "\n",
    "root_tk.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence.append('def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sequence[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"what's up\"]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.48215701e-07, 1.09632686e-03, 6.57407017e-13, 1.35297552e-04,\n",
       "        1.48766828e-12, 2.15338980e-04, 2.27178774e-08, 6.67586264e-10,\n",
       "        3.53283985e-10, 1.41789037e-11, 1.31535626e-05, 4.85195606e-10,\n",
       "        4.88582161e-11, 1.53317220e-11, 1.62022687e-11, 2.68347562e-12,\n",
       "        9.98539329e-01, 1.74824493e-07],\n",
       "       [2.73425282e-08, 8.47074203e-03, 1.83892207e-11, 4.81769057e-05,\n",
       "        9.06472206e-12, 1.92889464e-04, 4.11521967e-07, 1.11921483e-09,\n",
       "        9.47983292e-09, 4.26830515e-11, 6.75522870e-07, 1.32926328e-08,\n",
       "        1.88954785e-09, 6.74119538e-10, 1.08302853e-10, 4.98537878e-11,\n",
       "        9.91284490e-01, 2.58906243e-06],\n",
       "       [5.74702148e-20, 9.98860359e-01, 1.81268839e-22, 9.70396468e-06,\n",
       "        2.20775630e-16, 2.18191955e-04, 1.94481380e-13, 1.03755693e-16,\n",
       "        1.46037835e-10, 2.36989858e-14, 1.96111438e-13, 2.48757547e-15,\n",
       "        2.27704089e-14, 1.23836937e-14, 1.76316274e-21, 6.73573535e-19,\n",
       "        9.11873532e-04, 2.64505040e-09],\n",
       "       [3.04741055e-09, 6.15559020e-06, 5.60625473e-20, 1.33055175e-06,\n",
       "        2.73258394e-08, 3.01524528e-10, 1.51871800e-15, 1.52658686e-06,\n",
       "        8.72118176e-12, 9.99962687e-01, 2.74750382e-05, 1.11620531e-11,\n",
       "        5.72424199e-07, 1.18232570e-07, 1.63924230e-09, 3.72432183e-08,\n",
       "        1.62390439e-08, 2.69690315e-08],\n",
       "       [5.52907928e-11, 6.23579763e-05, 2.65205253e-14, 4.19671586e-08,\n",
       "        1.02209407e-04, 1.69996888e-08, 2.32116562e-10, 2.41540943e-09,\n",
       "        3.47042262e-09, 8.73845281e-08, 6.95332460e-07, 1.81722637e-09,\n",
       "        9.99819934e-01, 1.42030094e-05, 1.42550338e-09, 4.37174378e-07,\n",
       "        5.05658670e-09, 3.41745798e-10],\n",
       "       [3.92111760e-11, 4.71777275e-06, 9.48883486e-23, 9.99681234e-01,\n",
       "        1.09113003e-10, 3.13638477e-04, 2.68818569e-15, 2.93873870e-10,\n",
       "        2.41844085e-15, 2.67144031e-08, 3.07257693e-07, 1.09237456e-15,\n",
       "        2.90178309e-13, 1.17702194e-13, 1.69800460e-17, 1.19931932e-20,\n",
       "        2.68603872e-11, 2.58247295e-11],\n",
       "       [3.06363143e-13, 9.88366306e-01, 1.19197896e-15, 8.34112638e-04,\n",
       "        3.52686109e-11, 8.35211482e-03, 8.46339332e-10, 3.43939009e-11,\n",
       "        5.31206368e-08, 1.14609555e-09, 7.75030617e-09, 2.04093076e-10,\n",
       "        1.53189059e-10, 1.84968152e-10, 1.02540877e-14, 9.00152077e-14,\n",
       "        2.44685402e-03, 5.65338269e-07],\n",
       "       [3.36752331e-04, 1.34756174e-05, 6.45341934e-04, 2.51845086e-05,\n",
       "        1.01239948e-07, 4.38288134e-06, 8.46052527e-01, 9.33268201e-03,\n",
       "        4.09470249e-06, 7.55989262e-08, 4.39009964e-05, 9.75286064e-04,\n",
       "        1.45274644e-05, 1.25716333e-05, 1.40499999e-03, 5.88476157e-09,\n",
       "        4.17573028e-06, 1.41129836e-01],\n",
       "       [4.03969258e-09, 1.40657676e-05, 1.15141120e-07, 6.02848188e-11,\n",
       "        6.19279497e-12, 2.73375489e-11, 4.29101172e-04, 4.03544561e-07,\n",
       "        1.25948936e-01, 1.19671299e-07, 1.17368814e-04, 6.51103483e-06,\n",
       "        4.76310488e-05, 2.35800908e-05, 1.32383953e-03, 2.27168098e-01,\n",
       "        6.07861161e-01, 3.70591059e-02],\n",
       "       [6.01035499e-10, 3.98115496e-08, 1.65040537e-09, 3.86670100e-14,\n",
       "        3.66077064e-11, 4.12245615e-09, 3.59835468e-08, 6.69136730e-13,\n",
       "        2.65117872e-10, 3.69827002e-10, 2.38668162e-13, 7.43145823e-10,\n",
       "        6.44396758e-09, 9.99995589e-01, 6.09038098e-11, 1.00285424e-09,\n",
       "        8.27867837e-07, 3.56075293e-06],\n",
       "       [2.13710245e-15, 1.31493516e-06, 2.00857630e-18, 1.46119609e-11,\n",
       "        4.18081413e-07, 2.49584359e-12, 2.13437937e-13, 1.23619203e-12,\n",
       "        9.27183460e-13, 1.43353079e-10, 1.63501692e-08, 4.17014773e-12,\n",
       "        9.99998093e-01, 6.32596979e-08, 7.71398657e-12, 1.56190894e-09,\n",
       "        2.69840313e-12, 8.32292135e-14],\n",
       "       [7.34723782e-10, 1.14444640e-06, 1.43612649e-16, 2.54364732e-05,\n",
       "        4.66481068e-14, 9.99973416e-01, 2.52103941e-15, 3.78833977e-16,\n",
       "        4.15089356e-17, 1.07468401e-09, 1.79511828e-09, 1.67715538e-12,\n",
       "        3.24885721e-14, 2.23259009e-10, 5.94512946e-18, 1.38628247e-23,\n",
       "        2.49378339e-12, 2.71081380e-09],\n",
       "       [1.67027122e-06, 9.31647719e-08, 4.30669397e-23, 4.84911453e-08,\n",
       "        1.97796601e-09, 6.21619922e-10, 2.19405764e-20, 1.54604649e-12,\n",
       "        1.10341202e-16, 9.99996185e-01, 1.53351220e-06, 3.04670433e-13,\n",
       "        5.38074163e-09, 6.45374754e-10, 3.37137707e-10, 1.35760869e-08,\n",
       "        3.75326579e-07, 7.78959796e-12],\n",
       "       [2.22825602e-09, 4.85376290e-07, 3.66315915e-16, 6.76673695e-09,\n",
       "        9.99990463e-01, 4.54495925e-08, 1.16838895e-12, 2.39164979e-06,\n",
       "        3.56454948e-12, 4.29202544e-07, 6.81365432e-14, 3.70092734e-09,\n",
       "        6.16139369e-06, 2.38337639e-09, 9.42799875e-13, 3.09811725e-11,\n",
       "        1.73211713e-16, 4.58518779e-11],\n",
       "       [2.49476129e-09, 4.83315998e-05, 2.42783301e-07, 7.25312788e-09,\n",
       "        1.20432261e-10, 1.14211259e-10, 6.18923316e-03, 1.73656837e-04,\n",
       "        8.85910094e-01, 1.77792515e-07, 3.88108601e-04, 2.75132243e-06,\n",
       "        1.63076853e-04, 1.09191042e-05, 2.03155185e-04, 5.31563768e-03,\n",
       "        3.12514082e-02, 7.03432709e-02],\n",
       "       [1.88355003e-08, 7.05259083e-07, 1.03118332e-08, 2.98950575e-09,\n",
       "        1.51899019e-06, 3.39556919e-07, 8.17194268e-06, 2.04366870e-07,\n",
       "        3.25260849e-08, 9.67951678e-07, 1.38110270e-08, 3.43119780e-07,\n",
       "        4.38046518e-05, 9.99609888e-01, 1.16870835e-09, 3.59979246e-09,\n",
       "        2.35240645e-08, 3.33861361e-04],\n",
       "       [2.80556778e-09, 9.11140960e-05, 5.95513541e-08, 6.00680110e-08,\n",
       "        3.35229428e-10, 1.15781065e-10, 3.71259754e-03, 1.91438140e-03,\n",
       "        9.77940559e-01, 1.70094765e-07, 6.85749750e-04, 1.05494496e-06,\n",
       "        3.07976261e-05, 7.56640588e-07, 1.81135329e-04, 2.78386928e-04,\n",
       "        2.51331879e-03, 1.26498556e-02],\n",
       "       [1.45657282e-07, 4.38137278e-08, 2.44473012e-23, 5.48944623e-09,\n",
       "        6.36484254e-10, 6.56710589e-11, 1.94007668e-20, 1.13024781e-12,\n",
       "        4.53894040e-16, 9.99998808e-01, 9.98924520e-07, 5.88693428e-13,\n",
       "        3.34019723e-09, 1.19493182e-09, 2.07741491e-10, 2.92143056e-08,\n",
       "        4.38401422e-08, 1.93889436e-11],\n",
       "       [6.90975055e-12, 3.28187877e-03, 1.09073379e-16, 2.23264797e-04,\n",
       "        2.87975351e-12, 9.96494830e-01, 3.59247083e-13, 4.78001732e-14,\n",
       "        1.11525320e-11, 4.92909358e-09, 3.22685700e-08, 8.78144953e-12,\n",
       "        7.17137328e-14, 1.62507490e-11, 6.43893641e-16, 1.52905908e-18,\n",
       "        1.11794321e-07, 7.96817057e-11],\n",
       "       [1.52485995e-02, 5.65468294e-09, 9.28456187e-01, 1.12543750e-11,\n",
       "        1.24061213e-11, 4.82509734e-07, 3.46126892e-02, 1.32746045e-08,\n",
       "        1.63485017e-10, 1.37596558e-13, 2.64018446e-10, 1.78998392e-02,\n",
       "        8.76579875e-09, 7.39590718e-08, 1.76087872e-03, 3.83416937e-10,\n",
       "        1.06532261e-05, 2.01061182e-03],\n",
       "       [2.83590946e-08, 1.91948925e-06, 1.98165052e-15, 1.09917892e-07,\n",
       "        9.99977827e-01, 5.95037363e-07, 7.41620914e-12, 7.09589267e-06,\n",
       "        5.32055858e-11, 5.92123479e-06, 1.05403121e-12, 1.46847485e-08,\n",
       "        6.48787272e-06, 2.21554473e-08, 8.53716143e-12, 1.40435510e-10,\n",
       "        8.69645000e-15, 3.46678075e-10],\n",
       "       [3.78224740e-08, 1.24720420e-04, 2.34563258e-10, 1.94775159e-04,\n",
       "        1.48217111e-06, 6.64492958e-08, 7.56311056e-05, 9.91122961e-01,\n",
       "        1.93878368e-04, 2.35145868e-04, 2.56048108e-04, 5.15590614e-07,\n",
       "        2.00359682e-05, 2.22979579e-05, 2.00115346e-06, 4.99390005e-08,\n",
       "        2.73679291e-07, 7.75010325e-03],\n",
       "       [1.77196946e-09, 3.23415065e-06, 1.93022451e-20, 9.99882102e-01,\n",
       "        1.05866569e-07, 5.37127960e-07, 6.19305865e-12, 7.16824870e-05,\n",
       "        1.40155743e-11, 1.31373474e-06, 4.10309258e-05, 2.31180533e-14,\n",
       "        7.68949460e-10, 3.11755396e-12, 9.61453898e-13, 7.47564121e-16,\n",
       "        3.80220744e-10, 3.58613111e-10],\n",
       "       [1.07992927e-10, 9.80144143e-01, 2.47257414e-16, 1.88028626e-02,\n",
       "        8.33800805e-05, 3.48875401e-05, 3.89446003e-10, 3.62208607e-06,\n",
       "        5.93729851e-07, 2.17125689e-05, 2.05653232e-05, 9.48405354e-10,\n",
       "        8.52289842e-04, 9.10941509e-08, 1.04179762e-10, 1.63510683e-09,\n",
       "        3.57025783e-05, 5.82893556e-08],\n",
       "       [9.88044083e-01, 2.56814698e-13, 1.11824749e-02, 3.42693185e-17,\n",
       "        3.03859898e-15, 7.38599510e-08, 3.40012812e-05, 3.46753740e-14,\n",
       "        6.05764625e-14, 1.99178706e-17, 7.50887614e-16, 9.74231589e-05,\n",
       "        3.39942050e-13, 3.59836960e-08, 6.76676209e-05, 2.11776416e-10,\n",
       "        1.04797009e-05, 5.63668087e-04],\n",
       "       [5.78034420e-09, 6.70678844e-07, 7.09326843e-21, 9.99966502e-01,\n",
       "        6.64991306e-09, 1.75808873e-05, 1.86900503e-13, 1.57409303e-07,\n",
       "        4.10373301e-14, 3.46862208e-07, 1.47309484e-05, 1.03586581e-14,\n",
       "        3.64155546e-11, 3.34088074e-12, 2.19263525e-14, 1.22943904e-17,\n",
       "        2.91266511e-10, 3.73386842e-11],\n",
       "       [9.06765720e-08, 1.08118798e-03, 9.94820684e-11, 3.22898530e-04,\n",
       "        9.74455020e-07, 3.47169937e-08, 1.37766810e-05, 9.81339633e-01,\n",
       "        1.96879986e-03, 1.98080880e-03, 1.01180244e-02, 3.46697021e-07,\n",
       "        9.12765754e-05, 8.22814309e-06, 3.22363921e-05, 1.33847095e-06,\n",
       "        5.52752272e-06, 3.03496444e-03]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[np.argmax(res)] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_sequences,30,1662)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.532469e-04, 9.974770e-01, 1.969744e-03]], dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.expand_dims(X_test[0], axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
